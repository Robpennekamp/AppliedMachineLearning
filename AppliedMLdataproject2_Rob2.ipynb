{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6GgydedI6kRr"
   },
   "outputs": [],
   "source": [
    "import math, datetime, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import math, datetime, time, random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "def load_data(data_path):\n",
    "    data = pd.read_csv(data_path)  \n",
    "    return data\n",
    "\n",
    "inbound = load_data(\"inbound_loads.csv\")\n",
    "outbound = load_data(\"outbound_laods.csv\")\n",
    "weather = load_data(\"weather.csv\")\n",
    "#For loop to ensure that all pallet data is in the same dataframe\n",
    "pallet = load_data(\"Pallet_history_Gold_Spike[0].csv\")\n",
    "for x in range(1, 10):\n",
    "    pallet = pd.concat([pallet, load_data(f\"Pallet_history_Gold_Spike[{x}].csv\")])\n",
    "pallet = pallet.drop(['lot_code', \n",
    "                      'tran_type', \n",
    "                      'final_pallet_code', \n",
    "                      'warehouse_facility_id',\n",
    "                      'source_system_id'], axis=1)\n",
    "trainentest = load_data(\"demand_kWtrain_val.csv\")\n",
    "train = trainentest.iloc[:273988,:]\n",
    "test = trainentest.iloc[273988:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwIsOHLEjdRk"
   },
   "source": [
    "# Preprocessing the Data\n",
    "### 1. Load features and get them into the base dataframe\n",
    "\n",
    "- load all features into the base dataframe\n",
    "- create base_df\n",
    "\n",
    "### 2. Preprocessing of features\n",
    "- create dummy\n",
    "- normalization (DECISION: Min-Max Normalization) \n",
    "- one hot encoding\n",
    "- interpolating (fill in in between values)\n",
    "\n",
    "### 3. Cutting off the dataframe \n",
    "DECISION: ? \n",
    "Which datapoints to keep?\n",
    "Pallet_movement_5min starts 2nd of Jan 2019, so first rows is NaN. \n",
    "\n",
    "-  Demand values we have:\n",
    "12/31/2018  21:15:00  upandincluding 10/11/2021/ 6:07:00\n",
    "-  Predicting the demand from \n",
    "10/11/2021/ 6:08 upandincluding 12/13/2021  17:59:00 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load features and get them into the base dataframe\n",
    "#### Load features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total weight of pallets coming into the warehouse in the previous 1h, 5h, 10h, 23h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Incoming weight feature preprocessing\n",
    "#load all data\n",
    "incoming_weight_1h = load_data('incoming_weight_df_1h.csv')\n",
    "incoming_weight_5h = load_data('incoming_weight_df_5h.csv')\n",
    "incoming_weight_10h = load_data('incoming_weight_df_10h.csv')\n",
    "incoming_weight_23h = load_data('incoming_weight_df.csv')\n",
    "#rename columns \n",
    "incoming_weight_5h = incoming_weight_5h.rename(columns = {'weight_23h' : 'weight_5h'})\n",
    "incoming_weight_1h = incoming_weight_1h.rename(columns = {'weight_23h' : 'weight_1h'})\n",
    "incoming_weight_10h = incoming_weight_10h.rename(columns = {'weight_23h' : 'weight_10h'})\n",
    "#get them to a datetime object\n",
    "incoming_weight_1h['datetime_local'] = pd.to_datetime(incoming_weight_1h['datetime_local'])\n",
    "incoming_weight_5h['datetime_local'] = pd.to_datetime(incoming_weight_5h['datetime_local'])\n",
    "incoming_weight_10h['datetime_local'] = pd.to_datetime(incoming_weight_10h['datetime_local'])\n",
    "incoming_weight_23h['datetime_local'] = pd.to_datetime(incoming_weight_23h['datetime_local'])\n",
    "#set index to be datetime\n",
    "incoming_weight_1h.set_index('datetime_local', inplace=True)\n",
    "incoming_weight_5h.set_index('datetime_local', inplace=True)\n",
    "incoming_weight_10h.set_index('datetime_local', inplace=True)\n",
    "incoming_weight_23h.set_index('datetime_local', inplace=True)\n",
    "#reshape them to start at 2018-31-12 9:15PM\n",
    "incoming_weight_1h = incoming_weight_1h[2361:]\n",
    "incoming_weight_5h = incoming_weight_5h[2330:] \n",
    "incoming_weight_10h = incoming_weight_10h[2326:]\n",
    "incoming_weight_23h = incoming_weight_23h[2323:]\n",
    "#Drop duplicates\n",
    "incoming_weight_1h = incoming_weight_1h.loc[~incoming_weight_1h.index.duplicated()]\n",
    "incoming_weight_5h = incoming_weight_5h.loc[~incoming_weight_5h.index.duplicated()]\n",
    "incoming_weight_10h = incoming_weight_10h.loc[~incoming_weight_10h.index.duplicated()]\n",
    "incoming_weight_23h = incoming_weight_23h.loc[~incoming_weight_23h.index.duplicated()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of pallets moved within 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "pallet_move_5min = load_data('pallet_movement_5min_ft.csv')\n",
    "#rename column\n",
    "pallet_move_5min.rename(columns = {'quantity' : 'pallet_movement_5min'}, inplace = True)\n",
    "#make index a datetime object and set as index\n",
    "pallet_move_5min['datetime_local'] = pd.to_datetime(pallet_move_5min['datetime_local'])\n",
    "pallet_move_5min.set_index('datetime_local', inplace = True)\n",
    "#delete duplicates\n",
    "pallet_move_5min = pallet_move_5min.loc[~pallet_move_5min.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_door = load_data(\"feature_inbound_outbound_door_open.csv\")\n",
    "base_door = base_door.rename(columns = {'total' : 'doors_open'})\n",
    "base_door['datetime_local'] = pd.to_datetime(base_door['datetime_local'])\n",
    "base_door.set_index('datetime_local', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the base dataframe base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXuGO5sMNXm8",
    "outputId": "976536a7-441f-4770-c483-43a2861db264"
   },
   "outputs": [],
   "source": [
    "def addtimecol(df, colname): ####input df and colname \n",
    "    df[colname] = pd.to_datetime(df[colname])         \n",
    "    df['year'] = df[colname].dt.year\n",
    "    df['month'] = df[colname].dt.month\n",
    "    df['weekday'] = df[colname].dt.weekday\n",
    "    df['day'] = df[colname].dt.day\n",
    "    df['hour'] = df[colname].dt.hour\n",
    "    df['minute'] = df[colname].dt.minute        \n",
    "    return df\n",
    "\n",
    "#Create new dummy dfs\n",
    "base_df = train.copy()\n",
    "base_weather = weather.copy()\n",
    "\n",
    "#Remove unnecessary columns\n",
    "base_df = base_df.drop('Unnamed: 0', axis=1)\n",
    "base_weather = base_weather.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "base_weather[\"localstrptime\"]= pd.to_datetime(base_weather[\"localstrptime\"])\n",
    "base_df['datetime_local'] = pd.to_datetime(base_df['datetime_local'])\n",
    "base_weather = base_weather.rename(columns={'localstrptime':'datetime_local'})\n",
    "\n",
    "#Add time columns.\n",
    "addtimecol(base_df, 'datetime_local')\n",
    "\n",
    "#Set index to datetime\n",
    "base_df.set_index('datetime_local', inplace=True)\n",
    "base_weather.set_index('datetime_local', inplace=True)\n",
    "\n",
    "#Concatenate the weather DataFrame to the base DataFrame\n",
    "base_df = pd.concat([base_df, base_weather], axis=1)\n",
    "\n",
    "#Concatenate the incoming weight dataframe with the base dataframe\n",
    "base_df = pd.concat([base_df, incoming_weight_1h], axis=1)\n",
    "base_df = pd.concat([base_df, incoming_weight_5h], axis=1)\n",
    "base_df = pd.concat([base_df, incoming_weight_10h], axis=1)\n",
    "base_df = pd.concat([base_df, incoming_weight_23h], axis=1)\n",
    "\n",
    "#Concatenate the pallet movement feature\n",
    "base_df = pd.concat([base_df, pallet_move_5min], axis = 1)\n",
    "\n",
    "#Concatenate the door feature\n",
    "base_df = pd.concat([base_df, base_door], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing of features\n",
    "\n",
    "#### Creating the dummy df with which we will do model selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "5etBuOV3mniN",
    "outputId": "2346b0ac-3201-4761-8ba4-9e5f8c15be50"
   },
   "outputs": [],
   "source": [
    "dummy_df = base_df.copy()\n",
    "dummy_df = dummy_df.reset_index()\n",
    "dummy_df = dummy_df.drop(['hour'], axis=1)\n",
    "dummy_df = dummy_df.drop(['datetime'], axis=1)\n",
    "dummy_df = dummy_df.drop(['datetime_local'], axis=1)\n",
    "dummy_df = dummy_df.drop(['datetime_UTC'], axis=1)\n",
    "dummy_df\n",
    "\n",
    "dummy_normalized_df = dummy_df.copy()\n",
    "#Still drop demand_kW NaNs\n",
    "dummy_normalized_df = dummy_normalized_df.dropna(subset=['demand_kW'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1592126"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.pallet_movement_5min.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing functions defined\n",
    "DECISION: MinMax Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(df, columnname):\n",
    "    \"\"\"Function which returns a Dataframe where the given column is normalized through min-max normalization.\"\"\"\n",
    "    df[f'{columnname}_normalized'] = (df[columnname] - df[columnname].min()) / (df[columnname].max() - df[columnname].min())\n",
    "    return df.drop([columnname], axis=1)\n",
    "\n",
    "def add_one_hot_encoder(df, colname):\n",
    "    \"\"\"\n",
    "    Function which returns a DataFrame where the given column has been removed and replaced by\n",
    "    one-hot-encoding columns for each value in the original column.\n",
    "    \"\"\"\n",
    "    onehot = pd.get_dummies(df[colname], prefix=colname)\n",
    "    return df.drop(colname, axis=1).join(onehot)\n",
    "\n",
    "def interpolate_column(df, colname):\n",
    "    df[f'{colname}_interpolated'] = df[colname].interpolate(method='linear')\n",
    "    return df.drop([colname], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interpolate/one-hot-encoding/normalizing\n",
    "DECISION: ?  normalization of the doors_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate = ['Temperature', 'Relative Humidity']\n",
    "to_normalize = ['Relative Humidity_interpolated', 'Temperature_interpolated', 'weight_23h','weight_10h', 'weight_5h', 'weight_1h', 'pallet_movement_5min', 'doors_open']\n",
    "add_one_hot_encoding = ['weekday', 'year', 'month']\n",
    "\n",
    "\n",
    "for x in interpolate:\n",
    "    dummy_normalized_df = interpolate_column(dummy_normalized_df, x)\n",
    "for x in to_normalize:\n",
    "    dummy_normalized_df = normalize_column(dummy_normalized_df, x)\n",
    "for x in add_one_hot_encoding:\n",
    "    dummy_normalized_df = add_one_hot_encoder(dummy_normalized_df, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273879"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_normalized_df.pallet_movement_5min_normalized.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Model selection and building\n",
    "## 1. Importing model libraries\n",
    "\n",
    "## 2. Divide Train and Test data\n",
    "\n",
    "## 3. Test models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividing train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data, debug=False):\n",
    "    xtrain, ytrain, xtest, ytest = data\n",
    "    if debug:\n",
    "        print(\"Fitting model...\")\n",
    "    model = model.fit(xtrain, ytrain)\n",
    "    if debug:\n",
    "        print('Predicting...')\n",
    "    acc = model.predict(xtest)\n",
    "    acc2 = model.predict(xtrain)\n",
    "    if debug:\n",
    "        print('Calculating mean absolute error...')\n",
    "    return [mean_absolute_error(list(ytest), acc), mea model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dummy_normalized_df['pallet_movement_5min'] drop NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_normalized_df = dummy_normalized_df.dropna(subset = ['pallet_movement_5min_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define train, test sets\\n\",\n",
    "train, test = train_test_split(dummy_normalized_df, shuffle=True)\n",
    "X_train = train.copy().drop(['demand_kW'], axis=1)\n",
    "Y_train = train['demand_kW']\n",
    "X_test = test.copy().drop(['demand_kW'], axis=1)\n",
    "Y_test = test['demand_kW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Testing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_settings = {\n",
    "    'n_estimators': [20], #10, 20], 50],\n",
    "    'criterion': ['squared_error'] # friedman_mse squared_error, poisson', \n",
    "    #'max_depth': [2, 4, 6],\n",
    "    #'min_samples_split': [2, 4, 8]\n",
    "}\n",
    "data = [X_train, Y_train, X_test, Y_test]\n",
    "model = RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with parameters: {'n_estimators': 20, 'criterion': 'squared_error'}\n",
      "Mean Absolute Error = 68.50822690589105\n",
      "Time spent: 50.451395750045776 seconds. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_optimizer(data, model, params):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    parameter_combinations = itertools.product(*params.values())\n",
    "    results = {}\n",
    "    for parameters in parameter_combinations:\n",
    "        start = time.time()\n",
    "        params_dict = dict(zip(params.keys(), parameters))\n",
    "        print('Testing model with parameters: ' + str(params_dict))\n",
    "        current_model = model(**params_dict)\n",
    "        mae, model = test_model(current_model, data, debug = False)\n",
    "        print(f'Mean Absolute Error = {mae}')\n",
    "        modelname = str(params_dict)\n",
    "        end = time.time()\n",
    "        print(f'Time spent: {end-start} seconds. \\n')\n",
    "        results[modelname] = [mae, (end-start), model]\n",
    "    return results\n",
    "        \n",
    "test = model_optimizer(data, RandomForestRegressor, parameter_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1\n",
      "Training model...\n",
      "Predicting...\n",
      "Calculating rmse's...\n",
      "Fold 2 train error: 38.407309975338315. Test error: 618.3256080038326.\n",
      "Starting fold 2\n",
      "Training model...\n",
      "Predicting...\n",
      "Calculating rmse's...\n",
      "Fold 3 train error: 35.65644951732392. Test error: 542.6813514984067.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RandomForestRegressor(),\n",
       " 542.6813514984067,\n",
       " [38.407309975338315, 35.65644951732392],\n",
       " [618.3256080038326, 542.6813514984067]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "def run_timeseries_model(data, model, n_splits = 5, verbose=False):\n",
    "    \"\"\"\n",
    "    Function which receives a data and a model as input.\n",
    "    Can also receive the amount of splits you want as input.\n",
    "    For debugging purposes, set verbose to True.\n",
    "    \n",
    "    Outputs a list of:\n",
    "        - The resulting model\n",
    "        - The final RMSE\n",
    "        - A list of the resulting Train RMSEs (per fold)\n",
    "        - A list of the resulting Test RMSEs (per fold)\n",
    "    \"\"\"\n",
    "    \n",
    "    ts = TimeSeriesSplit(n_splits=n_splits)\n",
    "    count = 1\n",
    "    train_rmses = []\n",
    "    test_rmses = []\n",
    "    for train, test in ts.split(data):\n",
    "        if verbose:\n",
    "            print(f'Starting fold {count}')\n",
    "        cv_train, cv_test = data.iloc[train], data.iloc[test]\n",
    "        y_train = cv_train['demand_kW']\n",
    "        x_train = cv_train.drop(['demand_kW'], axis=1)\n",
    "        y_test = cv_test['demand_kW']\n",
    "        x_test = cv_test.drop(['demand_kW'], axis=1)\n",
    "        if verbose:\n",
    "            print(\"Training model...\")\n",
    "        model.fit(x_train, y_train)\n",
    "        if verbose:\n",
    "            print(\"Predicting...\")\n",
    "        y_pred_test = model.predict(x_test)\n",
    "        y_pred_train = model.predict(x_train)\n",
    "        if verbose:\n",
    "            print(\"Calculating rmse's...\")\n",
    "        train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "        test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "        train_rmses.append(train_rmse)\n",
    "        test_rmses.append(test_rmse)\n",
    "        if verbose:\n",
    "            print(f'Fold {count} train error: {train_rmse}. Test error: {test_rmse}.')\n",
    "        count += 1\n",
    "    return [model, test_rmse, train_rmses, test_rmses]\n",
    "\n",
    "run_timeseries_model(dummy_normalized_df, RandomForestRegressor(), n_splits=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data, model, parameters):\n",
    "    \"\"\"\n",
    "    Function which runs a model and outputs the following variables:\n",
    "    - Model.\n",
    "    - Root Mean Squared Error (RMSE)\n",
    "    - Time to train.\n",
    "    - Test error\n",
    "    - Train error\n",
    "    - Dict van parameters\n",
    "    \n",
    "    input variables:\n",
    "    data = Dataframe with all data\n",
    "    n_splits = Amount of Splits for Crossval\n",
    "    \n",
    "    \"\"\"\n",
    "    timeseries = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "    for train_indices, test_indices in timeseries.split(data):\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
